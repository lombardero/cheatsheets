---> PYTORCH <---

Note: Tensors are elements with multiple data on PyTorch, and are kept as a graph
      to optimize the computation of gradients, for example.
      On the PyTorch graph, each tensor is a node, and each new tensor created from
      another will be pointed with one edge of the graph. A backward graph will be
      created if the gradient is required (see Gradient part)


import torch

Tensors
    Creating
        t  = torch.tensor([1,2,3], device = torch.device("cpu"), dtype = torch.float)
            -> creates a 3x1 tensor (64-bit integer by default)
            -> device argument sets the device where PyTorch will run
               ("cpu" for the computer CPU, and "cuda:0" for a GPU)
            -> dtype argument sets the data tyoe of the data in the tensor
               (torch.float (32-bit), torch.float64 (or .double), torch.int32)
        t = torch.tensor(t, requires_grad = True)
            -> creates a tensor that can be derivable (PyTorch will be able to
               compute the gradient). What this does is tracking all Operations
               made on this tensor. If you create a new tensor y = t**2, it will
               keep track
        t = torch.tensor([1,2,3,4,5,6]).view(2,3)
            -> creates a tensor of 2 rows and 3 columns with the data inserted
        t = torch.linspace(init, end, n_elmts)
            -> creates a tensor of n_elmts elements, from init to end 
               (useful for plotting data, doing y = torch.f(t))
        t = torch.arange(init, end, step)
            -> creates a 1D tensor of the integers between init and end
               (end not included) - same as Python's range(init,end)
               (step = 1 as a Default)
        t = torch.from_numpy(np_array)
            -> converts the input numpy array np_array to a torch tensor
        np_array = t.numpy()
            -> converts a torch tensor into a numpy array
    Random
        t = torch.randn(row, col)
            -> creates a tensor of random numbers (normal distr, mean 0, std 1)
               of the rows and cols indicated
            -> dtype and device can be used
    Indexing
        t[idx_row, idx_col]
            -> returns the element in the index row and index column specified
               (Note: indexes start at 0, like Python)
        t[n_mat,idx_row,idx_col]
            -> returns the index row and column element on the nth matrix in
               the 3D tensor t
        t[n_mat, :, :]
            -> returns the nth matrix in the 3D tensor t
    Modifying
        t_mod  = t.view(row,col)
            -> creates a new reshaped tensor (without modifying t) with the
               specified numbers of rows and cols (gives error if dimension
               does not match input)
        t_mod = t.view(row,-1)
            -> reshapes the tensor with the specified number of rows, inferring
               the required number of columns
        t_mod = t.view(n_matrices, row, col)
            -> rearanges the data onto a 3-dimensional tensor of n 2-D n_matrices
               with the specified rows and columns
    Splicing / Joining
        torch.cat((t1,t2..),idx_colrow)
            -> concatenates tensors into a single tensor.
            -> fix idx_colrow as 0 to concatenate by row, and to 1 to concatenate
               by column
    Checking data types / indexes
        t.shape()
            -> returns the shape (size) of the tensor
        t.type()
            -> returns the tensor type of t
        t.dtype
            -> returns the type of data in the tensor (torch.int64) indicates
               a 64-bit integer
        t.dim()
            -> returns the dimension of the matrix (1 if vector, 2 if 2D matrix...)
    Operations
        t1 + t2
            -> returns the sum of individual components (t1 and t2 same size)
        torch.dot(t1, t2)
            -> computes the inner product (prod. escalar) of the two tensors
        torch.matmul(t1, t2)
            -> computes the matricial multiplication of tensors t1 and t2
            -> matmul, like Numpy, is a broadcasting method, meaning it will
               try to match
        torch.mm(t1, t2)
            -> computes matrix multiplication of t1 & t2
        t1.mm(t2)
            -> also computes matrix multiplication
    Transposing
        torch.transpose(t, 0, 1)
            -> transposes the dimensions specified (in this case row and col) for
               the tensor t
        torch.unsqueeze(t, dim)
            -> takes in a 1D tensor and returns it on the dimension specified
               (0 for row, 1 for column)
               (Sorts out error in mm)
    Functions
        torch.exp(t) / .sin(t) 
            -> applies exp(x) to each value of t (without modifying t)
    Gradient
      Note: PyTorch keeps a graph of all tensors created and multiplied. If the
            attribute requires_grad is True, all new tensors created from this
            one will also have the same option. When the option is activated,
            it will create a backward graph that will be used to compute the
            gradients once the backward function is called.
        t = torch.tensor(t, requires_grad = True)
            -> creates a tensor that can be derivable (PyTorch will be able to
               compute the gradient). What this does is tracking all Operations
               made on this tensor. If you create a new tensor y = t**2, it will
               keep track
        y = t * x
        y.backward()
            -> will compute the gradients of all upward nodes (in this case,
               the gradient of t is computed) on the graph, etting up the 
               variable grad to the upward nodes
        t.grad
            -> once the gradient is computed with the backward function, .grad
               outputs the gradient of tensor t (partial derivatives)
    Activation Functions
        t.clamp(min = 0)  (used as ReLu)
            -> outputs as zero all numbers below zero
               cann add a max = number variable to do the same with the upper bound
    
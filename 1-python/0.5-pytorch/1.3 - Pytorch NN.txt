---> PYTORCH NN LIBRARY <---

import torch.nn as nn

NN package allows to define a model as a sequence of 'nn objects',
which are a Tensor subclass that defines a single transformation
in the NN (both linear & non-linear). Torch provides an optimized
way for computing the gradient for these classes.

torch.nn.Module is the class type of all torch models; it supports
the following methods:
    modulename.state_dict()
        -> returns the state dictionnary of the module, which saves
           the status of the moduel. A useful key on the dictionnary
           is 'modulename.weight', which outputs the weights (in case
           it is a linear classifier)


CREATING MODELS
    Linear model
        lmodel = torch.nn.Linear(dim_in, dim_out)
            -> defines a linear model taking dim_in dimensions and
            'spitting' a dim_out vector

    Non-linearities
        relu = torch.nn.ReLU()
            -> applies element-wise ReLU function (keeps track of grad)
    
    Transformations
        Convolution
            conv = torch.nn.Conv2d(in_chnl, out_chnl, dim)
                -> creates a convolution layer with in_chnls specified (usually
                1 or 3), with dim*dim filters, outputting out_chnl feature maps
                -> note: sometimes dim is specified as kernel_size=dim
            conv = torch.nn.Conv2d(in_chnl, out_chnl, dim,
                                   stride=str, padding=pad)
                -> adds stride ('jump') str and padding (added pixels img) pad
    

    Sequence of models
        sequ_model = torch.nn.Sequential(lmodel, relu, ...)
            -> creates a model concatenating the models added
    
SYNTAX: PyTorch recommends creating a Model class for each neural network,
        as well as a forward pass for it:
    1) Create the model class with all transformations
    class Model(nn.Module):
        def __init__(self:)
            super(Model, self).__init__()
                -> creates a Model class inheriting the nn.Module class
                self.linear1 = nn.Linear(...)
                self.conv1 = nn.Conv2d(...)
                ...
    2) Create a forward pass function performing the forward pass 
    def forward(self, x):
        x = F.relu(self.linear1(x)) #first layer + non-linearity
        x = F.relu(self.conv1(x)) #second layer + non-linearity
        ...
        return x
    

LOSS FUNCTIONS
    MSE Loss
        loss_fn = toorch.nn.MSELoss()
            -> creates the loss function specified; the loss function will
            then require the user to input the predicted y ypred and
            the real y
    
    Computing the loss
        loss_fn(y_pred, y)
            -> returns the loss function for the inserted values (compares
            prediction and real)

OPTIMIZER: The optimizer package provides built-in optimizers (different
           ways of updating the weights)
    Creating the optimizer:
        ADAM
            optim = torch.optim.Adam(parameters, lr = lr)
                -> optimizes the the parameters of the model using the ADAM 
                optimizer and the learning rate lr
                    Note: parameters of the model can be called using 
                        model.parameters()
    Updating values
        optim.zero_grad()
            -> Note: before performing the backprop, need to call the 
              zerograd method to the optimizer so it does not add the 
              optimizer in the backward graph
        (loss.backward())
        optim.step()
            -> computes the step on the optimizer according to the computed
               gradient.
        